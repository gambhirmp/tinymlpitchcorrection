{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full Melody Pitch Correction (Causal TCN, Streaming)\n",
        "\n",
        "# 1) Imports & Config\n",
        "import os, json, glob, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ROOT = Path(\"/Users/mayagambhir/3600_final\")\n",
        "PROC_DIR = ROOT / \"data/processed/features\"\n",
        "META_DIR = ROOT / \"metadata\"\n",
        "OUT_DIR = ROOT / \"artifacts/full\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(META_DIR / \"feature_norm.json\", \"r\") as f:\n",
        "    norm = json.load(f)\n",
        "FEATURE_MEAN = np.array(norm[\"feature_mean\"] or [0.0]*64, dtype=np.float32)\n",
        "FEATURE_STD = np.array(norm[\"feature_std\"] or [1.0]*64, dtype=np.float32)\n",
        "FPS = norm.get(\"frames_per_second\", 100)\n",
        "\n",
        "N_MELS = 64\n",
        "SHIFT_RANGE_CENTS = 300.0\n",
        "\n",
        "random.seed(13)\n",
        "np.random.seed(13)\n",
        "tf.random.set_seed(13)\n",
        "print(\"Full melody config:\", dict(n_mels=N_MELS, fps=FPS))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Data loading (all categories), windowing, normalization\n",
        "\n",
        "def list_npz(split):\n",
        "    return sorted(glob.glob(str(PROC_DIR / split / '*.npz')))\n",
        "\n",
        "\n",
        "def normalize_features(x):\n",
        "    return (x - FEATURE_MEAN[None, :]) / (FEATURE_STD[None, :] + 1e-8)\n",
        "\n",
        "# Create overlapping windows for training with curriculum-ready augment hooks\n",
        "\n",
        "def make_sequences(npz_paths, T=120, stride=60, limit=None):\n",
        "    xs, ys = [], []\n",
        "    count = 0\n",
        "    for p in npz_paths:\n",
        "        arr = np.load(p)\n",
        "        logmel = normalize_features(arr['logmel'].astype(np.float32))\n",
        "        target_shift = arr['target_shift'].astype(np.float32)\n",
        "        L = len(target_shift)\n",
        "        i = 0\n",
        "        while i + T <= L:\n",
        "            xs.append(logmel[i:i+T])\n",
        "            ys.append(target_shift[i:i+T, None])\n",
        "            i += stride\n",
        "            count += 1\n",
        "            if limit and count >= limit:\n",
        "                break\n",
        "        if limit and count >= limit:\n",
        "            break\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "train_files = list_npz('train')\n",
        "val_files = list_npz('val')\n",
        "X_train, y_train = make_sequences(train_files, T=120, stride=60)\n",
        "X_val, y_val = make_sequences(val_files, T=120, stride=60)\n",
        "print('Shapes:', X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "BATCH=32\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(4096).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Causal TCN with dilations (depthwise-separable)\n",
        "from tensorflow.keras import layers as L, models\n",
        "\n",
        "INPUT_DIM = N_MELS\n",
        "\n",
        "inp = L.Input(shape=(None, INPUT_DIM), name='features')\n",
        "x = L.Conv1D(64, 1, padding='causal', activation='relu')(inp)\n",
        "\n",
        "# Residual dilated blocks\n",
        "for d in [1, 2, 4, 8]:\n",
        "    res = x\n",
        "    x = L.DepthwiseConv1D(kernel_size=3, dilation_rate=d, padding='causal', activation=None)(x)\n",
        "    x = L.Conv1D(64, 1, activation='relu')(x)\n",
        "    x = L.Add()([x, res])\n",
        "\n",
        "x = L.Conv1D(64, 1, activation='relu')(x)\n",
        "shift = L.Conv1D(1, 1, activation=None, name='shift_cents')(x)\n",
        "conf = L.Conv1D(1, 1, activation='sigmoid', name='confidence')(x)\n",
        "\n",
        "model = models.Model(inputs=inp, outputs=[shift, conf])\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss={'shift_cents':'mae','confidence':'binary_crossentropy'},\n",
        "    loss_weights={'shift_cents':1.0,'confidence':0.01},\n",
        "    metrics={'shift_cents':'mae'}\n",
        ")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Training with curriculum hook (chromatic -> scale-aware)\n",
        "\n",
        "def pack_labels(y):\n",
        "    conf = np.ones_like(y, dtype=np.float32)\n",
        "    return (y, conf)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_shift_cents_mae', factor=0.5, patience=3, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_shift_cents_mae', patience=7, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds.map(lambda x,y: (x, pack_labels(y))),\n",
        "    validation_data=(X_val, pack_labels(y_val)),\n",
        "    epochs=40,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(history.history['shift_cents_mae'], label='train MAE')\n",
        "plt.plot(history.history['val_shift_cents_mae'], label='val MAE')\n",
        "plt.legend(); plt.title('Cents MAE'); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Streaming emulator (per-hop inference)\n",
        "\n",
        "class StreamingEmulator:\n",
        "    def __init__(self, model, feature_mean, feature_std):\n",
        "        self.model = model\n",
        "        self.fm = feature_mean.astype(np.float32)\n",
        "        self.fs = feature_std.astype(np.float32)\n",
        "\n",
        "    def step(self, frame):\n",
        "        # frame: [64]\n",
        "        x = (frame[None, None, :].astype(np.float32) - self.fm[None,None,:]) / (self.fs[None,None,:] + 1e-8)\n",
        "        shift, conf = self.model.predict(x, verbose=0)\n",
        "        return float(shift[0,0,0]), float(conf[0,0,0])\n",
        "\n",
        "# Example: run over first validation clip frame-by-frame\n",
        "if len(val_files) > 0:\n",
        "    arr = np.load(val_files[0])\n",
        "    frames = arr['logmel']\n",
        "    sim = StreamingEmulator(model, FEATURE_MEAN, FEATURE_STD)\n",
        "    out_shift = []\n",
        "    out_conf = []\n",
        "    for t in range(frames.shape[0]):\n",
        "        s,c = sim.step(frames[t])\n",
        "        out_shift.append(s); out_conf.append(c)\n",
        "    print('Streaming run frames:', len(out_shift))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Export to SavedModel and TFLite (int8)\n",
        "\n",
        "export_dir = OUT_DIR / 'saved_model'\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sig_inputs = tf.keras.Input(shape=(None, N_MELS), name='features', dtype=tf.float32)\n",
        "sig_shift, sig_conf = model(sig_inputs)\n",
        "serve = tf.keras.Model(inputs=sig_inputs, outputs={'shift_cents': sig_shift, 'confidence': sig_conf})\n",
        "\n",
        "serve.save(export_dir, include_optimizer=False)\n",
        "print('SavedModel exported to', export_dir)\n",
        "\n",
        "# Representative dataset windows\n",
        "\n",
        "def rep_ds():\n",
        "    for _ in range(256):\n",
        "        i = np.random.randint(0, len(X_train))\n",
        "        x = X_train[i:i+1].astype(np.float32)\n",
        "        yield [x]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(str(export_dir))\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = rep_ds\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "(OUT_DIR / 'tflite').mkdir(parents=True, exist_ok=True)\n",
        "with open(OUT_DIR / 'tflite' / 'full_melody.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print('TFLite model written.')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

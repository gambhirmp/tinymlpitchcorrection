{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline single‑note pitch correction — notebook guide\n",
        "This notebook trains the lightweight baseline model that predicts per‑frame pitch correction (cents) for sustained notes.\n",
        "\n",
        "Outline:\n",
        "1. Imports & config — paths, normalization stats, constants.\n",
        "2. Load data — build `[T,64]` mel sequences and labels from `.npz`.\n",
        "3. Model — tiny GRU with heads for `shift_cents` and `confidence`.\n",
        "4. Train — MAE loss on `shift_cents` vs `target_shift` + early stopping.\n",
        "5. Export — SavedModel for local checks and TFLite for mobile.\n",
        "\n",
        "Tip: 100 cents = 1 semitone. Reported MAE is in cents (lower is better).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline: Single-Note Pitch Correction (Streaming-friendly)\n",
        "\n",
        "# 1) Imports & Config\n",
        "import os, json, math, glob, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ROOT = Path(\"/Users/mayagambhir/3600_final\")\n",
        "PROC_DIR = ROOT / \"data/processed/features\"\n",
        "META_DIR = ROOT / \"metadata\"\n",
        "OUT_DIR = ROOT / \"artifacts/baseline\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(META_DIR / \"feature_norm.json\", \"r\") as f:\n",
        "    norm = json.load(f)\n",
        "FEATURE_MEAN = np.array(norm[\"feature_mean\"] or [0.0]*64, dtype=np.float32)\n",
        "FEATURE_STD = np.array(norm[\"feature_std\"] or [1.0]*64, dtype=np.float32)\n",
        "FPS = norm.get(\"frames_per_second\", 100)\n",
        "\n",
        "N_MELS = 64\n",
        "FRAME_HOP_MS = 10\n",
        "SHIFT_RANGE_CENTS = 300.0\n",
        "\n",
        "random.seed(13)\n",
        "np.random.seed(13)\n",
        "tf.random.set_seed(13)\n",
        "\n",
        "print(\"Config:\", dict(n_mels=N_MELS, fps=FPS, shift_range=SHIFT_RANGE_CENTS))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load data (single‑note sequences)\n",
        "This prepares training/validation windows from the processed dataset.\n",
        "- Load `.npz` files from `data/processed/features/{train,val}`.\n",
        "- Filter to sustained notes via a simple stability heuristic on `target_shift` over voiced frames.\n",
        "- Normalize mel frames using `feature_norm.json` mean/std.\n",
        "- Slice overlapping windows (T=100 frames, stride=50) to build `[batch, T, 64]` inputs and `[batch, T, 1]` labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Data loading: Single-note segments\n",
        "\n",
        "def list_npz(split):\n",
        "    return sorted(glob.glob(str(PROC_DIR / split / '*.npz')))\n",
        "\n",
        "# Heuristic single-note filter: low stddev of target_shift and f0 drift\n",
        "# This picks stable sustained notes for the baseline\n",
        "\n",
        "def is_single_note(arr, thr_cents_std=15.0):\n",
        "    ts = arr['target_shift'].astype(np.float32)\n",
        "    voiced = arr['voiced'].astype(np.float32) > 0.5\n",
        "    if voiced.sum() < 20:\n",
        "        return False\n",
        "    ts_v = ts[voiced]\n",
        "    return np.std(ts_v) < thr_cents_std\n",
        "\n",
        "\n",
        "def normalize_features(x):\n",
        "    return (x - FEATURE_MEAN[None, :]) / (FEATURE_STD[None, :] + 1e-8)\n",
        "\n",
        "# Build sequences of length T frames\n",
        "\n",
        "def make_sequences(npz_paths, T=100, stride=50, limit=None):\n",
        "    xs, ys = [], []\n",
        "    count = 0\n",
        "    for p in npz_paths:\n",
        "        arr = np.load(p)\n",
        "        if not is_single_note(arr):\n",
        "            continue\n",
        "        logmel = normalize_features(arr['logmel'].astype(np.float32))\n",
        "        target_shift = arr['target_shift'].astype(np.float32)\n",
        "        L = len(target_shift)\n",
        "        i = 0\n",
        "        while i + T <= L:\n",
        "            xs.append(logmel[i:i+T])           # [T, 64]\n",
        "            ys.append(target_shift[i:i+T, None])  # [T, 1]\n",
        "            i += stride\n",
        "            count += 1\n",
        "            if limit and count >= limit:\n",
        "                break\n",
        "        if limit and count >= limit:\n",
        "            break\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "train_files = list_npz('train')\n",
        "val_files = list_npz('val')\n",
        "print(f\"Found train files={len(train_files)}, val files={len(val_files)}\")\n",
        "\n",
        "X_train, y_train = make_sequences(train_files, T=100, stride=50, limit=None)\n",
        "X_val, y_val = make_sequences(val_files, T=100, stride=50, limit=None)\n",
        "print('Shapes:', X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "BATCH=32\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(2048).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Model (tiny GRU baseline)\n",
        "A small causal sequence model that maps mel frames → per‑frame pitch correction in cents.\n",
        "- Input: `[batch, T, 64]` normalized log‑mels.\n",
        "- Backbone: `GRU(64, return_sequences=True)` with small dense layers.\n",
        "- Heads: \n",
        "  - `shift_cents` (linear) — main regression target (`target_shift`)\n",
        "  - `confidence` (sigmoid) — optional gating head (baseline uses ones)\n",
        "Rationale: GRU captures slow pitch trend with near‑zero lookahead and few parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Model: Tiny GRU (causal-ish) predicting shift_cents and confidence\n",
        "\n",
        "from tensorflow.keras import layers as L, models\n",
        "\n",
        "INPUT_DIM = N_MELS\n",
        "HIDDEN = 64\n",
        "\n",
        "inputs = L.Input(shape=(None, INPUT_DIM), name='features')\n",
        "x = L.Dense(64, activation='relu')(inputs)\n",
        "x = L.GRU(HIDDEN, return_sequences=True, name='gru1')(x)\n",
        "x = L.Dense(64, activation='relu')(x)\n",
        "shift = L.Dense(1, activation=None, name='shift_cents')(x)  # linear\n",
        "conf = L.Dense(1, activation='sigmoid', name='confidence')(x)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=[shift, conf])\n",
        "\n",
        "# Loss: MAE on shift; small smoothness penalty could be added later\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss={'shift_cents': 'mae', 'confidence': 'binary_crossentropy'},\n",
        "    loss_weights={'shift_cents': 1.0, 'confidence': 0.01},\n",
        "    metrics={'shift_cents': 'mae'}\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Train and validate\n",
        "We optimize mean absolute error (MAE) on `shift_cents` vs. `target_shift` and a tiny auxiliary BCE on a dummy `confidence` head.\n",
        "- `train_mapped`: uses `tf.ones_like(y)` as confidence for all voiced frames (baseline simplification).\n",
        "- Callbacks: ReduceLROnPlateau + EarlyStopping on `val_shift_cents_mae` (lower is better).\n",
        "- The plot shows train/val MAE in cents (100 cents = 1 semitone).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Training loop\n",
        "\n",
        "# For validation (numpy arrays), create ones via numpy\n",
        "def pack_val(y):\n",
        "    conf = np.ones_like(y, dtype=np.float32)\n",
        "    return (y, conf)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_shift_cents_mae', mode='min', factor=0.5, patience=3, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_shift_cents_mae', mode='min', patience=7, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Use tf.ones_like inside the dataset map to avoid NumPy on Tensors\n",
        "train_mapped = train_ds.map(lambda x, y: (x, (y, tf.ones_like(y))))\n",
        "\n",
        "history = model.fit(\n",
        "    train_mapped,\n",
        "    validation_data=(X_val, pack_val(y_val)),\n",
        "    epochs=30,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(history.history['shift_cents_mae'], label='train MAE')\n",
        "plt.plot(history.history['val_shift_cents_mae'], label='val MAE')\n",
        "plt.legend(); plt.title('Cents MAE'); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Export (SavedModel + TFLite)\n",
        "This section wraps the trained model in a simple serving signature and exports:\n",
        "- SavedModel (used for local numeric checks and plotting)\n",
        "- TFLite (mobile deployment; baseline GRU uses Select TF Ops)\n",
        "\n",
        "Notes:\n",
        "- The representative dataset is used only for int8 PTQ; in this baseline we keep Select TF Ops for GRU.\n",
        "- Metadata such as feature dims and hop are stored separately in `metadata/feature_norm.json` and `metadata/tflite_metadata.json`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Export to SavedModel and TFLite (int8)\n",
        "\n",
        "export_dir = OUT_DIR / 'saved_model'\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Wrap for fixed-shape TFLite export\n",
        "sig_inputs = tf.keras.Input(shape=(None, N_MELS), name='features', dtype=tf.float32)\n",
        "sig_shift, sig_conf = model(sig_inputs)\n",
        "serve = tf.keras.Model(inputs=sig_inputs, outputs={'shift_cents': sig_shift, 'confidence': sig_conf})\n",
        "\n",
        "serve.export(export_dir)\n",
        "print('SavedModel exported to', export_dir)\n",
        "\n",
        "# Representative dataset: feature windows\n",
        "\n",
        "def rep_ds():\n",
        "    for _ in range(200):\n",
        "        idx = np.random.randint(0, len(X_train))\n",
        "        x = X_train[idx:idx+1].astype(np.float32)\n",
        "        yield [x]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(str(export_dir))\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Keep float model and allow Select TF Ops to support GRU export\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "(OUT_DIR / 'tflite').mkdir(exist_ok=True, parents=True)\n",
        "with open(OUT_DIR / 'tflite' / 'baseline_single_note.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print('TFLite model written.')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
